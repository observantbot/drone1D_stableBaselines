{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drone1D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Control of drone in 1-D using Reinforcement Learning"
      ],
      "metadata": {
        "id": "kj4VFT9HjjxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries or Framework selection"
      ],
      "metadata": {
        "id": "zQZSp9Xtjcyv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afRCvedn_RI_"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drone Envioronment"
      ],
      "metadata": {
        "id": "5DCVP72ahmgP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jznv1rlGaNFj"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pybullet as p\n",
        "\n",
        "\n",
        "class Drone1DEnv(gym.Env):\n",
        "    metadata = {'render.modes':['human']}\n",
        "\n",
        "\n",
        "    def __init__(self, drone):\n",
        "        super(Drone1DEnv, self).__init__()\n",
        "\n",
        "        self.observation_space = spaces.Box(low=-1, high=1,\n",
        "                                            shape=(2,), \n",
        "                                            dtype=np.float32)\n",
        "\n",
        "        self.action_space = spaces.Box(low = -1, high = 1,\n",
        "                                       shape = (1,), \n",
        "                                       dtype=np.float32)\n",
        "\n",
        "        self.drone = drone\n",
        "        self.z_des = 8.0      # m\n",
        "        self.z_dot_des = 0.0  # m/s\n",
        "        self.mass = 1.5       # kg\n",
        "        self.gravity = 9.81   # m/s^2\n",
        "        self.obs_max = 5.0\n",
        "\n",
        "\n",
        "    # current state--> current position, current velocity in z-direction\n",
        "    def state(self):\n",
        "        drone_pos, _ = p.getBasePositionAndOrientation(self.drone)\n",
        "        drone_vel, _ = p.getBaseVelocity(self.drone)\n",
        "\n",
        "        # discretization and error representation of state\n",
        "        state = self.abs_to_error_state(drone_pos[2], drone_vel[2])\n",
        "        return state\n",
        "\n",
        "\n",
        "    # reward\n",
        "    def reward(self, action):\n",
        "        e_z, e_z_dot = self.state()\n",
        "        reward = 0.0\n",
        "        if self.done():\n",
        "            if (abs(e_z)>=1.5 or abs(e_z_dot)>=1):\n",
        "                reward = -5.0\n",
        "                print('...outbound conditions...', e_z, e_z_dot)\n",
        "            else:\n",
        "                reward = 100.0\n",
        "                print('----desired point achieved----')\n",
        "        else:            \n",
        "            reward = -(10*abs(e_z) + 0.5*abs(e_z_dot) + 0.3*abs(action))\n",
        "\n",
        "        return float(reward)\n",
        "    \n",
        "    \n",
        "    # whether goal is achieved or not.\n",
        "    def done(self):\n",
        "        e_z, e_z_dot = self.state()\n",
        "        '''\n",
        "        done=1; episodes terminates when:\n",
        "          1. if e_z >= 1.5: drone is 1.5*5 m or more apart from desired pos.\n",
        "          2. if e_z_dot >= 1: drone velocity is >=5 m/s {pysical constraints}.\n",
        "          3. if e_z<=0.01/5.0 and e_z_dot<=0.01/5: desired condition is achieved.\n",
        "        '''\n",
        "        if (abs(e_z)>=1.5 or abs(e_z_dot)>=1) or\\\n",
        "            (abs(e_z)<=0.01/self.obs_max\\\n",
        "             and abs(e_z_dot)<=0.01/self.obs_max):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    #info\n",
        "    def info(self):\n",
        "        return {}\n",
        "\n",
        "\n",
        "    # step\n",
        "    def step(self, action):\n",
        "        # action must be a float.\n",
        "        action_ = (action+1)*self.mass*self.gravity\n",
        "        p.applyExternalForce(objectUniqueId=self.drone, linkIndex=-1,\n",
        "                         forceObj=[0, 0 ,action_], posObj=[0,0,0], \n",
        "                         flags=p.LINK_FRAME)\n",
        "        p.stepSimulation()\n",
        "        state = self.state()\n",
        "        reward = self.reward(action)\n",
        "        done = self.done()\n",
        "        info = self.info()\n",
        "        return state, reward, done, info\n",
        "\n",
        "\n",
        "    # reset the environment\n",
        "    def reset(self):\n",
        "        # initializing quadcopter with random z_position and z_velocity\n",
        "        droneStartPos, droneStartOrn, droneStartLinVel, droneStartAngVel\\\n",
        "             = self.random_state_generator()\n",
        "        p.resetBasePositionAndOrientation(self.drone, droneStartPos,\n",
        "                                          droneStartOrn)\n",
        "        p.resetBaseVelocity(self.drone, droneStartLinVel, \n",
        "                            droneStartAngVel)\n",
        "        # print(\"\\n[--------z_des: %f,    z_init: %f,     v_init: %f ---------]\\n\\n\"\n",
        "        #       %(self.z_des, droneStartPos[2], droneStartLinVel[2]))\n",
        "\n",
        "        # return state\n",
        "        state  = self.abs_to_error_state(droneStartPos[2], droneStartLinVel[2])\n",
        "        return state\n",
        "\n",
        "\n",
        "    def random_state_generator(self):\n",
        "        # initialize drone's position between 3 and 13 m.\n",
        "        z_init = random.uniform(3.0,13.0) \n",
        "        # initialized with velocity in between -1 and 1 m/s.\n",
        "        z_dot_init = random.uniform(-1,1)\n",
        "        \n",
        "        StartPos = [0,0,z_init] \n",
        "        StartOrn = p.getQuaternionFromEuler([0,0,0])\n",
        "        StartLinVel = [0,0,z_dot_init]\n",
        "        StartAngVel = [0,0,0]\n",
        "        return StartPos, StartOrn, StartLinVel, StartAngVel\n",
        "\n",
        "\n",
        "    def abs_to_error_state(self, z, z_dot):\n",
        "        e_z = (z - self.z_des) / self.obs_max\n",
        "        e_z_dot = (z_dot - self.z_dot_des) / self.obs_max\n",
        "\n",
        "        return np.array([e_z, e_z_dot])\n",
        "\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integration with pybullet simulation"
      ],
      "metadata": {
        "id": "CrrX-mdghsyM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0c05mMsaNB_"
      },
      "source": [
        "import pybullet as p\n",
        "import pybullet_data\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def init_simulation(render = False):\n",
        "\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print('optimized...')\n",
        "\n",
        "    if render:\n",
        "        physicsClient = p.connect(p.GUI)\n",
        "    else:\n",
        "        physicsClient = p.connect(p.DIRECT)\n",
        "        \n",
        "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
        "    p.setGravity(0,0,-9.81)\n",
        "    p.setTimeStep(0.01)\n",
        "\n",
        "    '------------------------------------'\n",
        "    # drone\n",
        "    drone = p.loadURDF('/content/drive/MyDrive/drone_URDF/drone.urdf')\n",
        "\n",
        "    # marker at desired point\n",
        "    sphereVisualId = p.createVisualShape(shapeType=p.GEOM_SPHERE,\n",
        "                                        radius = 0.05,\n",
        "                                        rgbaColor= [1, 0, 0, 1])\n",
        "    marker = p.createMultiBody(baseMass=0.0, baseCollisionShapeIndex=-1,\n",
        "                    baseVisualShapeIndex=sphereVisualId, basePosition=[0, 0, 8.0],\n",
        "                    useMaximalCoordinates=False)\n",
        "    '-------------------------------------'\n",
        "\n",
        "    return drone\n",
        "\n",
        "def end_simulation():\n",
        "    p.disconnect()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensure our Custom Enviornment matches OpenAI gym interface"
      ],
      "metadata": {
        "id": "sy75DFVBhy4o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvoDVlyNaM_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9863d0fc-742a-4f23-f03d-bf2b4e1bde83"
      },
      "source": [
        "# ensure custom environment matches gym env interface \n",
        "from stable_baselines.common.env_checker import check_env\n",
        "\n",
        "drone = init_simulation()\n",
        "env = Drone1DEnv(drone)\n",
        "print('obs_space',env.observation_space)\n",
        "print('action_spc: ', env.action_space)\n",
        "\n",
        "for i in range(10):\n",
        "    print(env.action_space.sample())\n",
        "\n",
        "check_env(env, warn=True)\n",
        "end_simulation()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimized...\n",
            "obs_space Box(-1.0, 1.0, (2,), float32)\n",
            "action_spc:  Box(-1.0, 1.0, (1,), float32)\n",
            "[0.1728102]\n",
            "[-0.74861497]\n",
            "[-0.9111984]\n",
            "[0.41781712]\n",
            "[0.40568385]\n",
            "[0.84027106]\n",
            "[0.2439733]\n",
            "[0.98645836]\n",
            "[-0.85671103]\n",
            "[-0.2155102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training of DDPG model using Stable Baselines"
      ],
      "metadata": {
        "id": "EhNP9-ONiJj5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J6tpOEAaM82"
      },
      "source": [
        "# train a DDPG algorithm based model using Stable Baselines library from open AI.\n",
        "from stable_baselines.ddpg.policies import MlpPolicy\n",
        "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines import DDPG\n",
        "\n",
        "drone = init_simulation()\n",
        "env = Drone1DEnv(drone)\n",
        "param_noise = None\n",
        "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(1), sigma=float(0.5) * np.ones(1))\n",
        "\n",
        "model = DDPG(MlpPolicy, env, verbose=1, param_noise=param_noise,\n",
        "             action_noise=action_noise)\n",
        "\n",
        "for i in range(1, 51):\n",
        "  save_path = \"/content/drive/MyDrive/drone1D/drone\" + str(i) + \".zip\"\n",
        "  if i==1:\n",
        "    model.learn(total_timesteps=10000)\n",
        "    # at every 10000 time steps, we will save our model\n",
        "    model.save(save_path)\n",
        "  else:\n",
        "    del model\n",
        "    model = DDPG.load(prev_path, env)\n",
        "    model.learn(total_timesteps=10000)\n",
        "    model.save(save_path)\n",
        "  prev_path = save_path\n",
        "\n",
        "print('done')\n",
        "\n",
        "end_simulation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking which model is working the best with manually chosen games"
      ],
      "metadata": {
        "id": "thcaZcX7iTT8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFpy12_rogQ4"
      },
      "source": [
        "# from pybulletSim import init_simulation, end_simulation\n",
        "# from drone1D_env import Drone1DEnv\n",
        "from stable_baselines import DDPG\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "drone = init_simulation(render = False)\n",
        "env = Drone1DEnv(drone)\n",
        "\n",
        "\n",
        "def run_agent(model):\n",
        "    obs = env.reset()\n",
        "    print('initial state: ', obs)\n",
        "    t = 0\n",
        "    for i in range(500):\n",
        "        action = model.predict(obs)\n",
        "        obs, reward, done, _ = env.step(action[0])\n",
        "        # time.sleep(0.01)\n",
        "        t += 0.01\n",
        "        if done:\n",
        "            if reward>90.0:\n",
        "                print('***********',t)\n",
        "            else:\n",
        "                print('***********', 5)     # evaluation metric\n",
        "                t = 5\n",
        "            return t\n",
        "    return t\n",
        "\n",
        "t1 = []\n",
        "game = []\n",
        "for i in range(1,51):\n",
        "    filepath = '/content/drive/MyDrive/drone1D/drone'+ str(i) + '.zip'\n",
        "    model = DDPG.load(filepath)\n",
        "    t = run_agent(model=model)\n",
        "    t1.append(t)\n",
        "    game.append(i)\n",
        "\n",
        "\n",
        "d = {'game': game, 'time': t1}\n",
        "df = pd.DataFrame(d).to_csv('/content/drive/MyDrive/drone1D/game6.csv')\n",
        "\n",
        "\n",
        "end_simulation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run our Trained Agent on random games"
      ],
      "metadata": {
        "id": "lyPD5Kx-i5uQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTuoJMvCaM6r"
      },
      "source": [
        "# run our best trained agent on random games\n",
        "from stable_baselines import DDPG\n",
        "\n",
        "drone = init_simulation(render = False)\n",
        "env = Drone1DEnv(drone)\n",
        "model = DDPG.load('/content/drive/MyDrive/drone1D/ddpg_drone2.4D.zip')    # path to the best model\n",
        "\n",
        "\n",
        "obs = env.reset()\n",
        "print('initial state: ', obs)\n",
        "\n",
        "for _ in range(10000):\n",
        "    action = model.predict(obs)\n",
        "    obs, rew, done, info = env.step(action[0])\n",
        "\n",
        "    if done:\n",
        "        env.reset()\n",
        "\n",
        "end_simulation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvmq5bS9aM4M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8nHI-7__fXi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56TSKXDx_fRu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}